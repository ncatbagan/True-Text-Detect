<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About - AI Models</title>
    <link rel="stylesheet" href="/static/styles.css">
    <script src="script.js" defer></script>
</head>
<body>
    <div class="navbar">
        <img id="mqicon" src="/static/images/icon.png" alt="Icon"/>
        <div class="navbar-right">
            <a href="/home" class="nav-link">Home</a>
            <div class="dropdown">
                <button class="nav-link dropdown-button">About</button>
                <div class="dropdown-content">
                    <a href="/about/purpose">Purpose</a>
                    <a href="/about/ai-models">AI Models</a>                    
                </div>
            </div>
            <a href="/logs" class="nav-link">Logs</a>
        </div>
    </div>
    <div class="aicontainer">
        <h1>AI Models</h1>
        <div class="section">
            <h3>OpenAI API (Fine-tuned GPT-4o-mini)</h3>
            <b>Overview:</b><br>
            The OpenAI API used in this project is the GPT-4o-mini(“o” for “omni”) model, which is OpenAI's most advanced model in the small models category [3]. The model has been fine-tuned specifically for the task of AI-generated text detection.
            <br>
            <br><b>Fine-tuning Details:</b><br>
            This model was fine-tuned using a dataset of 100 text samples: 50 generated by AI systems and 50 written by humans. The dataset was curated to cover various topics and writing styles, ensuring the model can generalise well across diverse content. The fine-tuning process involved optimising the model's parameters to distinguish between AI-written and human-written content more effectively.
        </div>
        <hr>
        <div class="section">
            <h3>Gemini API (Gemini 1.5 Flash)</h3>
            <b>Overview:</b><br>
            The Gemini API is another large language model employed in this project. Gemini 1.5 Flash is a fast and versatile multimodal model [1].
            <br>
            <br><b>Fine-tuning Details:</b><br>
            This model has not been fine-tuned yet; however, it will be fine-tuned in the future using the same dataset as the other models.
        </div>
        <hr>
        <div class="section">
            <h3>BERT from Huggingface (Fine-tuned)</h3>
            <b>Overview:</b><br>
            BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model pre-trained on a large text corpus [2]. The BERT model used in this project is sourced from Huggingface and has been fine-tuned to detect AI-generated text.
            <br>
            <br><b>Fine-tuning Details:</b><br>
            Like the OpenAI model, BERT was fine-tuned using the same dataset of 100 text samples (50 AI-written and 50 human-written). Fine-tuning this specific dataset allows BERT to become specialised in recognising subtle differences in sentence structure, word choice, and coherence that distinguish AI-generated content from human-written text.
        </div>
        <hr>
        <div class="section">
            <h3>References:</h3>
            [1] "Gemini API documentation," Google AI, [Online]. Available: https://ai.google.dev/gemini-api/docs. [Accessed: 04-Oct-2024].<br>
            [2] "Google BERT: BERT-base uncased model," Hugging Face, [Online]. Available: https://huggingface.co/google-bert/bert-base-uncased. [Accessed: 04-Oct-2024].<br>
            [3] "GPT-4o-mini model," OpenAI, [Online]. Available: https://platform.openai.com/docs/models/gpt-4o-mini. [Accessed: 04-Oct-2024].
        </div>
    </div>
</body>
</html>
